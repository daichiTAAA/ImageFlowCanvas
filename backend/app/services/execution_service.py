from typing import List, Optional, Dict, Any
from fastapi import UploadFile
import uuid
from datetime import datetime, timezone
from app.models.execution import (
    Execution,
    ExecutionRequest,
    ExecutionStatus,
    ExecutionProgress,
)
from app.services.file_service import FileService
from app.services.kafka_service import KafkaService
from app.services.grpc_pipeline_executor import get_grpc_pipeline_executor
import uuid
import asyncio
import os
import logging

logger = logging.getLogger(__name__)

# ã‚°ãƒ­ãƒ¼ãƒãƒ« ExecutionService ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_execution_service_instance = None


def get_global_execution_service():
    """ã‚°ãƒ­ãƒ¼ãƒãƒ« ExecutionService ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—"""
    global _execution_service_instance
    if _execution_service_instance is None:
        _execution_service_instance = ExecutionService()
    return _execution_service_instance


class ExecutionService:
    def __init__(self):
        self.executions = {}
        self.file_service = FileService()
        self.kafka_service = KafkaService()
        self.grpc_executor = get_grpc_pipeline_executor()
        self.dev_mode = os.getenv("DEV_MODE", "true").lower() == "true"
        self.websocket_manager = None  # é…å»¶åˆæœŸåŒ–

    def get_websocket_manager(self):
        """WebSocket ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã‚’é…å»¶åˆæœŸåŒ–"""
        if self.websocket_manager is None:
            try:
                # ã‚°ãƒ­ãƒ¼ãƒãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆmain.py ã§ä½œæˆã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ï¼‰
                import app.main

                self.websocket_manager = app.main.manager
            except Exception as e:
                print(f"Could not get WebSocket manager: {e}")
                self.websocket_manager = None
        return self.websocket_manager

    async def execute_pipeline(
        self, execution_request: ExecutionRequest, input_files: List[UploadFile]
    ) -> Execution:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ"""
        execution_id = str(uuid.uuid4())

        # å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆå®Ÿè¡ŒIDã‚’ä½¿ç”¨ï¼‰
        uploaded_files = []
        for i, file in enumerate(input_files):
            # å®Ÿè¡ŒIDãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚¡ã‚¤ãƒ«IDã‚’ç”Ÿæˆï¼ˆè¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«å¯¾å¿œï¼‰
            file_id = (
                f"{execution_id}-input-{i}" if len(input_files) > 1 else execution_id
            )
            actual_file_id = await self.file_service.upload_file(file, file_id)
            uploaded_files.append(actual_file_id)

        # å®Ÿè¡Œã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ
        execution = Execution(
            execution_id=execution_id,
            pipeline_id=execution_request.pipeline_id,
            progress=ExecutionProgress(
                current_step="åˆæœŸåŒ–ä¸­",
                total_steps=1,  # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã®è©³ç´°ã‹ã‚‰è¨ˆç®—ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
                completed_steps=0,
                percentage=0.0,
            ),
        )

        self.executions[execution_id] = execution

        # Execute pipeline directly using gRPC services
        # This eliminates the 750-1450ms overhead and achieves 40-100ms processing
        try:
            # uploaded_files now contains actual MinIO object names
            # Start direct gRPC pipeline execution in background
            asyncio.create_task(
                self._execute_pipeline_direct(
                    execution_id, execution_request, uploaded_files
                )
            )
            logger.info(f"Started direct gRPC pipeline execution for {execution_id}")
        except Exception as e:
            logger.error(f"Failed to start direct pipeline execution: {e}")
            # Fallback to Kafka-based execution if direct execution fails
            try:
                message = {
                    "execution_id": execution_id,
                    "pipeline_id": execution_request.pipeline_id,
                    "input_files": uploaded_files,
                    "parameters": execution_request.parameters,
                    "priority": execution_request.priority,
                }
                await self.kafka_service.send_message(
                    "image-processing-requests", message
                )
                logger.info(
                    f"Fallback: sent execution to Kafka queue for {execution_id}"
                )
            except Exception as kafka_error:
                logger.error(
                    f"Both direct execution and Kafka fallback failed: {kafka_error}"
                )
                execution.status = ExecutionStatus.FAILED

        return execution

    async def _execute_pipeline_direct(
        self,
        execution_id: str,
        execution_request: ExecutionRequest,
        input_files: List[str],
    ):
        """
        Execute pipeline directly using gRPC calls (40-100ms processing time)
        Ultra-fast execution using direct gRPC service calls
        """
        try:
            execution = self.executions.get(execution_id)
            if not execution:
                logger.error(f"Execution {execution_id} not found")
                return

            # Update status to running
            execution.status = ExecutionStatus.RUNNING
            execution.started_at = datetime.now(timezone.utc)
            execution.progress.current_step = "å‡¦ç†é–‹å§‹"
            await self._notify_execution_update(execution)

            # Create pipeline configuration from execution request
            pipeline_config = await self._build_pipeline_config(
                execution_request, input_files
            )

            # Execute pipeline using direct gRPC calls
            result = await self.grpc_executor.execute_pipeline(
                pipeline_config, execution_id
            )

            # Update execution with results
            if result["status"] == "completed":
                execution.status = ExecutionStatus.COMPLETED
                execution.progress.current_step = "å®Œäº†"
                execution.progress.completed_steps = execution.progress.total_steps
                execution.progress.percentage = 100.0
                execution.completed_at = datetime.now(timezone.utc)

                # Add execution steps based on pipeline config
                from app.models.execution import ExecutionStep, OutputFile, StepStatus
                import os

                pipeline_config = await self._build_pipeline_config(
                    execution_request, input_files
                )
                execution.steps = []

                for step_config in pipeline_config.get("steps", []):
                    step = ExecutionStep(
                        step_id=step_config["stepId"],
                        name=step_config.get("componentName", step_config["stepId"]),
                        component_name=step_config.get(
                            "componentName", step_config["stepId"]
                        ),
                        status=StepStatus.COMPLETED,
                        started_at=execution.started_at,
                        completed_at=execution.completed_at,
                    )
                    execution.steps.append(step)

                # Add output files from all processing steps
                output_files = []

                # Add results from each step
                if "results" in result:
                    print(f"ğŸ” Found {len(result['results'])} step results")
                    for step_id, step_result in result["results"].items():
                        print(f"ğŸ” Processing step {step_id}: {step_result}")
                        if "output_path" in step_result and step_result["output_path"]:
                            output_path = step_result["output_path"]
                            filename = os.path.basename(output_path)

                            if filename:
                                # å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’å–å¾—
                                file_size, content_type = await self._get_file_info(
                                    output_path
                                )

                                # MinIOã«ä¿å­˜ã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰ã‚’file_idã¨ã—ã¦ä½¿ç”¨
                                file_id = os.path.splitext(filename)[0]

                                output_file = OutputFile(
                                    file_id=file_id,
                                    filename=filename,
                                    file_size=file_size,
                                    content_type=content_type,
                                )
                                output_files.append(output_file)

                        # Check for additional files in metadata (e.g., JSON detection files)
                        if "metadata" in step_result:
                            metadata = step_result["metadata"]
                            print(f"ğŸ” Found metadata: {metadata}")
                            if "json_output_file" in metadata:
                                json_filename = metadata["json_output_file"]
                                json_content_type = metadata.get(
                                    "json_content_type", "application/json"
                                )

                                print(f"ğŸ¯ Processing JSON file: {json_filename}")

                                # Get JSON file info
                                json_file_size, _ = await self._get_file_info(
                                    json_filename
                                )
                                # Use full filename as file_id for JSON files to avoid conflicts
                                json_file_id = json_filename

                                json_output_file = OutputFile(
                                    file_id=json_file_id,
                                    filename=json_filename,
                                    file_size=json_file_size,
                                    content_type=json_content_type,
                                )
                                output_files.append(json_output_file)
                                print(f"âœ… Added JSON file: {json_filename}")
                            else:
                                print("âŒ No json_output_file in metadata")
                        else:
                            print("âŒ No metadata in step_result")

                # Add final output if available and not already included
                if "final_output_path" in result and result["final_output_path"]:
                    final_path = result["final_output_path"]
                    filename = os.path.basename(final_path)

                    if filename:
                        # Check if this file is not already in output_files
                        existing_files = [f.filename for f in output_files]
                        if filename not in existing_files:
                            file_size, content_type = await self._get_file_info(
                                final_path
                            )

                            # MinIOã«ä¿å­˜ã•ã‚Œã‚‹ãƒ•ã‚¡ã‚¤ãƒ«åï¼ˆæ‹¡å¼µå­ãªã—ï¼‰ã‚’file_idã¨ã—ã¦ä½¿ç”¨
                            file_id = os.path.splitext(filename)[0]

                            output_file = OutputFile(
                                file_id=file_id,
                                filename=filename,
                                file_size=file_size,
                                content_type=content_type,
                            )
                            output_files.append(output_file)

                execution.output_files = output_files

                logger.info(
                    f"Direct gRPC pipeline execution completed for {execution_id} in {result.get('execution_time_ms', 0):.2f}ms"
                )

            else:
                execution.status = ExecutionStatus.FAILED
                execution.error_message = result.get("error", "Unknown error")
                logger.error(
                    f"Direct gRPC pipeline execution failed for {execution_id}: {execution.error_message}"
                )

            await self._notify_execution_update(execution)

        except Exception as e:
            logger.error(f"Error in direct pipeline execution for {execution_id}: {e}")
            execution = self.executions.get(execution_id)
            if execution:
                execution.status = ExecutionStatus.FAILED
                execution.error_message = str(e)
                await self._notify_execution_update(execution)

    async def _build_pipeline_config(
        self, execution_request: ExecutionRequest, input_files: List[str]
    ) -> Dict[str, Any]:
        """Build pipeline configuration for direct gRPC execution"""
        from app.services.pipeline_service import PipelineService
        from app.database import get_db

        # Get actual pipeline definition from database
        pipeline_service = PipelineService()
        db_gen = get_db()
        db = await db_gen.__anext__()

        try:
            pipeline = await pipeline_service.get_pipeline(
                execution_request.pipeline_id, db
            )

            if not pipeline:
                # Fallback to default pipeline if not found
                logger.warning(
                    f"Pipeline {execution_request.pipeline_id} not found, using fallback"
                )
                return self._get_fallback_pipeline_config(
                    execution_request, input_files
                )

            # Build steps from pipeline components in the defined order
            steps = []
            for i, component in enumerate(pipeline.components):
                step_id = f"{component.component_type}-step-{i}"

                # Merge component parameters with execution request parameters
                merged_parameters = component.parameters.copy()
                merged_parameters.update(execution_request.parameters)

                # Set up dependencies based on component order
                dependencies = []
                if i > 0:
                    # Each component depends on the previous one (simple sequential execution)
                    previous_component = pipeline.components[i - 1]
                    dependencies = [f"{previous_component.component_type}-step-{i-1}"]

                step = {
                    "stepId": step_id,
                    "componentName": component.component_type,
                    "parameters": merged_parameters,
                    "dependencies": dependencies,
                }
                steps.append(step)

            return {
                "pipelineId": execution_request.pipeline_id,
                "steps": steps,
                "globalParameters": {
                    "inputPath": input_files[0] if input_files else None,
                    "executionId": execution_request.pipeline_id,
                },
            }

        except Exception as e:
            logger.error(f"Error building pipeline config: {e}")
            return self._get_fallback_pipeline_config(execution_request, input_files)
        finally:
            await db.close()

    def _get_fallback_pipeline_config(
        self, execution_request: ExecutionRequest, input_files: List[str]
    ) -> Dict[str, Any]:
        """Fallback pipeline configuration"""
        return {
            "pipelineId": execution_request.pipeline_id,
            "steps": [
                {
                    "stepId": "resize-step",
                    "componentName": "resize",
                    "parameters": {
                        "width": execution_request.parameters.get("target_width", 800),
                        "height": execution_request.parameters.get(
                            "target_height", 600
                        ),
                        "maintain_aspect_ratio": True,
                    },
                    "dependencies": [],
                },
                {
                    "stepId": "ai-detection-step",
                    "componentName": "ai_detection",
                    "parameters": {
                        "model_name": execution_request.parameters.get(
                            "model_name", "yolo"
                        ),
                        "confidence_threshold": execution_request.parameters.get(
                            "confidence_threshold", 0.5
                        ),
                        "draw_boxes": True,
                    },
                    "dependencies": ["resize-step"],
                },
                {
                    "stepId": "filter-step",
                    "componentName": "filter",
                    "parameters": {
                        "filter_type": execution_request.parameters.get(
                            "filter_type", "gaussian"
                        ),
                        "intensity": execution_request.parameters.get(
                            "filter_intensity", 1.0
                        ),
                    },
                    "dependencies": ["ai-detection-step"],
                },
            ],
            "globalParameters": {
                "inputPath": input_files[0] if input_files else None,
                "executionId": execution_request.pipeline_id,
            },
        }

    async def _notify_execution_update(self, execution: Execution):
        """Notify WebSocket clients of execution updates"""
        websocket_manager = self.get_websocket_manager()
        if websocket_manager:
            try:
                # Convert execution to dict for broadcasting
                execution_dict = execution.dict()
                await websocket_manager.broadcast_execution_update(
                    execution.execution_id, execution_dict
                )
            except Exception as e:
                logger.warning(f"Failed to broadcast execution update: {e}")

    async def _get_file_info(self, file_path: str) -> tuple[int, str]:
        """MinIOã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’å–å¾—"""
        try:
            # FileServiceã‚’ä½¿ç”¨ã—ã¦MinIOã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ã‚’å–å¾—
            file_info = await self.file_service.get_file_info(file_path)

            if file_info:
                file_size = file_info.get("size", 0)
                content_type = file_info.get("content_type", "application/octet-stream")

                # ãƒ•ã‚¡ã‚¤ãƒ«æ‹¡å¼µå­ã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’æ¨å®šï¼ˆMinIOã‹ã‚‰å–å¾—ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
                if content_type == "application/octet-stream":
                    content_type = self._guess_content_type_from_filename(file_path)

                return file_size, content_type
            else:
                # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±ãŒå–å¾—ã§ããªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                return 0, self._guess_content_type_from_filename(file_path)

        except Exception as e:
            logger.warning(f"Failed to get file info for {file_path}: {e}")
            # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰æ¨å®š
            return 0, self._guess_content_type_from_filename(file_path)

    def _guess_content_type_from_filename(self, filename: str) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‚’æ¨å®š"""
        import os

        ext = os.path.splitext(filename)[1].lower()
        content_type_map = {
            ".jpg": "image/jpeg",
            ".jpeg": "image/jpeg",
            ".png": "image/png",
            ".gif": "image/gif",
            ".bmp": "image/bmp",
            ".webp": "image/webp",
            ".tiff": "image/tiff",
            ".tif": "image/tiff",
            ".svg": "image/svg+xml",
            ".json": "application/json",
            ".txt": "text/plain",
            ".csv": "text/csv",
            ".xml": "application/xml",
        }

        return content_type_map.get(ext, "application/octet-stream")  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚’å¤‰æ›´

    async def get_execution(self, execution_id: str) -> Optional[Execution]:
        """å®Ÿè¡ŒçŠ¶æ³ã‚’å–å¾—"""
        return self.executions.get(execution_id)

    async def cancel_execution(self, execution_id: str) -> bool:
        """å®Ÿè¡Œã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«"""
        execution = self.executions.get(execution_id)
        if not execution:
            return False

        if execution.status in [ExecutionStatus.PENDING, ExecutionStatus.RUNNING]:
            execution.status = ExecutionStatus.CANCELLED

            # Kafkaã«ã‚­ãƒ£ãƒ³ã‚»ãƒ«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€ä¿¡
            cancel_message = {"execution_id": execution_id, "action": "cancel"}
            await self.kafka_service.send_message("execution-control", cancel_message)

            return True
        return False

    async def get_executions(
        self, limit: int = 100, offset: int = 0, pipeline_id: Optional[str] = None
    ) -> List[Execution]:
        """å®Ÿè¡Œå±¥æ­´ã‚’å–å¾—ï¼ˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³IDã§ãƒ•ã‚£ãƒ«ã‚¿å¯èƒ½ï¼‰"""
        all_executions = list(self.executions.values())

        # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³IDã§ãƒ•ã‚£ãƒ«ã‚¿
        if pipeline_id:
            all_executions = [
                execution
                for execution in all_executions
                if execution.pipeline_id == pipeline_id
            ]

        # ä½œæˆæ—¥æ™‚ã§ã‚½ãƒ¼ãƒˆï¼ˆæ–°ã—ã„é †ï¼‰
        all_executions.sort(key=lambda x: x.created_at, reverse=True)
        return all_executions[offset : offset + limit]

    async def get_pending_executions(self) -> List[Execution]:
        """å®Ÿè¡Œå¾…ã¡ã®å®Ÿè¡Œã‚’å–å¾—ï¼ˆç›´æ¥å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ç”¨ï¼‰"""
        pending_executions = []
        for execution in self.executions.values():
            if execution.status == ExecutionStatus.PENDING:
                pending_executions.append(execution)
        return pending_executions

    async def get_running_executions(self) -> List[Execution]:
        """å®Ÿè¡Œä¸­ã®å®Ÿè¡Œã‚’å–å¾—ï¼ˆãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç›£è¦–ç”¨ï¼‰"""
        running_executions = []
        for execution in self.executions.values():
            if execution.status == ExecutionStatus.RUNNING:
                running_executions.append(execution)
        return running_executions

    async def update_execution_status(
        self,
        execution_id: str,
        status: ExecutionStatus,
        progress_data: Optional[dict] = None,
    ):
        """å®Ÿè¡ŒçŠ¶æ³ã‚’æ›´æ–°ï¼ˆKafkaã‚³ãƒ³ã‚·ãƒ¥ãƒ¼ãƒãƒ¼ã‹ã‚‰å‘¼ã³å‡ºã•ã‚Œã‚‹ï¼‰"""
        execution = self.executions.get(execution_id)
        if not execution:
            return

        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹å¤‰æ›´æ™‚ã«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æ›´æ–°
        from datetime import datetime

        if execution.status != status:
            if status == ExecutionStatus.RUNNING and execution.started_at is None:
                execution.started_at = datetime.now(timezone.utc)
            elif status in [
                ExecutionStatus.COMPLETED,
                ExecutionStatus.FAILED,
                ExecutionStatus.CANCELLED,
            ]:
                # å®Œäº†æ™‚ã«started_atãŒè¨­å®šã•ã‚Œã¦ã„ãªã„å ´åˆã¯è¨­å®š
                if execution.started_at is None:
                    execution.started_at = datetime.now(timezone.utc)
                execution.completed_at = datetime.now(timezone.utc)

        execution.status = status

        if progress_data:
            execution.progress.current_step = progress_data.get(
                "current_step", execution.progress.current_step
            )
            execution.progress.completed_steps = progress_data.get(
                "completed_steps", execution.progress.completed_steps
            )
            execution.progress.percentage = progress_data.get(
                "percentage", execution.progress.percentage
            )

        # WebSocket ã§é€²æ—ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ é€šçŸ¥
        websocket_manager = self.get_websocket_manager()
        if websocket_manager:
            try:
                update_data = {
                    "execution_id": execution_id,
                    "status": status.value,
                    "progress": {
                        "current_step": execution.progress.current_step,
                        "completed_steps": execution.progress.completed_steps,
                        "total_steps": execution.progress.total_steps,
                        "percentage": execution.progress.percentage,
                    },
                }
                await websocket_manager.send_execution_update(execution_id, update_data)
            except Exception as e:
                print(f"Failed to send WebSocket update: {e}")

    async def fix_execution_timestamps(self, execution_id: str) -> bool:
        """æ—¢å­˜å®Ÿè¡Œã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ä¿®æ­£ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰"""
        execution = self.executions.get(execution_id)
        if not execution:
            return False

        from datetime import datetime

        # ã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰å®Ÿè¡Œæ™‚é–“ã‚’æ¨å®š
        if execution.steps and execution.status == ExecutionStatus.COMPLETED:
            step_start_times = [
                datetime.fromisoformat(s.started_at.replace("Z", "+00:00"))
                for s in execution.steps
                if s.started_at and s.status == "completed"
            ]
            step_end_times = [
                datetime.fromisoformat(s.completed_at.replace("Z", "+00:00"))
                for s in execution.steps
                if s.completed_at and s.status == "completed"
            ]

            if step_start_times and step_end_times:
                execution.started_at = min(step_start_times).replace(tzinfo=None)
                execution.completed_at = max(step_end_times).replace(tzinfo=None)
                return True

        return False
