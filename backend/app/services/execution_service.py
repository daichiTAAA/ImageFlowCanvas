from typing import List, Optional, Dict, Any
from fastapi import UploadFile
import uuid
from datetime import datetime, timezone
from app.models.execution import (
    Execution,
    ExecutionRequest,
    ExecutionStatus,
    ExecutionProgress,
)
from app.services.file_service import FileService
from app.services.kafka_service import KafkaService
from app.services.grpc_pipeline_executor import get_grpc_pipeline_executor
import uuid
import asyncio
import os
import logging

logger = logging.getLogger(__name__)

# „Ç∞„É≠„Éº„Éê„É´ ExecutionService „Ç§„É≥„Çπ„Çø„É≥„Çπ
_execution_service_instance = None


def get_global_execution_service():
    """„Ç∞„É≠„Éº„Éê„É´ ExecutionService „Ç§„É≥„Çπ„Çø„É≥„Çπ„ÇíÂèñÂæó"""
    global _execution_service_instance
    if _execution_service_instance is None:
        _execution_service_instance = ExecutionService()
    return _execution_service_instance


class ExecutionService:
    def __init__(self):
        self.executions = {}
        self.file_service = FileService()
        self.kafka_service = KafkaService()
        self.grpc_executor = get_grpc_pipeline_executor()
        self.dev_mode = os.getenv("DEV_MODE", "true").lower() == "true"
        self.websocket_manager = None  # ÈÅÖÂª∂ÂàùÊúüÂåñ

    def get_websocket_manager(self):
        """WebSocket „Éû„Éç„Éº„Ç∏„É£„Éº„ÇíÈÅÖÂª∂ÂàùÊúüÂåñ"""
        if self.websocket_manager is None:
            try:
                # „Ç∞„É≠„Éº„Éê„É´„Ç§„É≥„Çπ„Çø„É≥„Çπ„ÇíÂèñÂæóÔºàmain.py „Åß‰ΩúÊàê„Åï„Çå„Å¶„ÅÑ„Çã„ÇÇ„ÅÆÔºâ
                import app.main

                self.websocket_manager = app.main.manager
            except Exception as e:
                print(f"Could not get WebSocket manager: {e}")
                self.websocket_manager = None
        return self.websocket_manager

    async def execute_pipeline(
        self, execution_request: ExecutionRequest, input_files: List[UploadFile]
    ) -> Execution:
        """„Éë„Ç§„Éó„É©„Ç§„É≥„ÇíÂÆüË°å"""
        execution_id = str(uuid.uuid4())

        # ÂÖ•Âäõ„Éï„Ç°„Ç§„É´„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„ÉâÔºàÂÆüË°åID„Çí‰ΩøÁî®Ôºâ
        uploaded_files = []
        for i, file in enumerate(input_files):
            # ÂÆüË°åID„Éô„Éº„Çπ„ÅÆ„Éï„Ç°„Ç§„É´ID„ÇíÁîüÊàêÔºàË§áÊï∞„Éï„Ç°„Ç§„É´ÂØæÂøúÔºâ
            file_id = (
                f"{execution_id}-input-{i}" if len(input_files) > 1 else execution_id
            )
            actual_file_id = await self.file_service.upload_file(file, file_id)
            uploaded_files.append(actual_file_id)

        # ÂÆüË°å„Ç™„Éñ„Ç∏„Çß„ÇØ„Éà„Çí‰ΩúÊàê
        execution = Execution(
            execution_id=execution_id,
            pipeline_id=execution_request.pipeline_id,
            progress=ExecutionProgress(
                current_step="ÂàùÊúüÂåñ‰∏≠",
                total_steps=1,  # „Éë„Ç§„Éó„É©„Ç§„É≥„ÅÆË©≥Á¥∞„Åã„ÇâË®àÁÆó„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çã
                completed_steps=0,
                percentage=0.0,
            ),
        )

        self.executions[execution_id] = execution

        # Execute pipeline directly using gRPC services
        # This eliminates the 750-1450ms overhead and achieves 40-100ms processing
        try:
            # uploaded_files now contains actual MinIO object names
            # Start direct gRPC pipeline execution in background
            asyncio.create_task(
                self._execute_pipeline_direct(
                    execution_id, execution_request, uploaded_files
                )
            )
            logger.info(f"Started direct gRPC pipeline execution for {execution_id}")
        except Exception as e:
            logger.error(f"Failed to start direct pipeline execution: {e}")
            # Fallback to Kafka-based execution if direct execution fails
            try:
                message = {
                    "execution_id": execution_id,
                    "pipeline_id": execution_request.pipeline_id,
                    "input_files": uploaded_files,
                    "parameters": execution_request.parameters,
                    "priority": execution_request.priority,
                }
                await self.kafka_service.send_message(
                    "image-processing-requests", message
                )
                logger.info(
                    f"Fallback: sent execution to Kafka queue for {execution_id}"
                )
            except Exception as kafka_error:
                logger.error(
                    f"Both direct execution and Kafka fallback failed: {kafka_error}"
                )
                execution.status = ExecutionStatus.FAILED

        return execution

    async def _execute_pipeline_direct(
        self,
        execution_id: str,
        execution_request: ExecutionRequest,
        input_files: List[str],
    ):
        """
        Execute pipeline directly using gRPC calls (40-100ms processing time)
        Ultra-fast execution using direct gRPC service calls
        """
        try:
            execution = self.executions.get(execution_id)
            if not execution:
                logger.error(f"Execution {execution_id} not found")
                return

            # Update status to running
            execution.status = ExecutionStatus.RUNNING
            execution.started_at = datetime.now(timezone.utc)
            execution.progress.current_step = "Âá¶ÁêÜÈñãÂßã"
            await self._notify_execution_update(execution)

            # Create pipeline configuration from execution request
            pipeline_config = await self._build_pipeline_config(
                execution_request, input_files
            )

            # Execute pipeline using direct gRPC calls
            result = await self.grpc_executor.execute_pipeline(
                pipeline_config, execution_id
            )

            # Update execution with results
            if result["status"] == "completed":
                execution.status = ExecutionStatus.COMPLETED
                execution.progress.current_step = "ÂÆå‰∫Ü"
                execution.progress.completed_steps = execution.progress.total_steps
                execution.progress.percentage = 100.0
                execution.completed_at = datetime.now(timezone.utc)

                # Add execution steps based on pipeline config
                from app.models.execution import ExecutionStep, OutputFile, StepStatus
                import os

                pipeline_config = await self._build_pipeline_config(
                    execution_request, input_files
                )
                execution.steps = []

                for step_config in pipeline_config.get("steps", []):
                    step = ExecutionStep(
                        step_id=step_config["stepId"],
                        name=step_config.get("componentName", step_config["stepId"]),
                        component_name=step_config.get(
                            "componentName", step_config["stepId"]
                        ),
                        status=StepStatus.COMPLETED,
                        started_at=execution.started_at,
                        completed_at=execution.completed_at,
                    )
                    execution.steps.append(step)

                # Add output files from all processing steps
                output_files = []

                # Add results from each step
                if "results" in result:
                    print(f"üîç Found {len(result['results'])} step results")
                    for step_id, step_result in result["results"].items():
                        print(f"üîç Processing step {step_id}: {step_result}")
                        if "output_path" in step_result and step_result["output_path"]:
                            output_path = step_result["output_path"]
                            filename = os.path.basename(output_path)

                            if filename:
                                # ÂÆüÈöõ„ÅÆ„Éï„Ç°„Ç§„É´„Çµ„Ç§„Ç∫„Å®„Ç≥„É≥„ÉÜ„É≥„ÉÑ„Çø„Ç§„Éó„ÇíÂèñÂæó
                                file_size, content_type = await self._get_file_info(
                                    output_path
                                )

                                # MinIO„Å´‰øùÂ≠ò„Åï„Çå„Çã„Éï„Ç°„Ç§„É´ÂêçÔºàÊã°ÂºµÂ≠ê„Å™„ÅóÔºâ„Çífile_id„Å®„Åó„Å¶‰ΩøÁî®
                                file_id = os.path.splitext(filename)[0]

                                output_file = OutputFile(
                                    file_id=file_id,
                                    filename=filename,
                                    file_size=file_size,
                                    content_type=content_type,
                                )
                                output_files.append(output_file)

                        # Check for additional files in metadata (e.g., JSON detection files)
                        if "metadata" in step_result:
                            metadata = step_result["metadata"]
                            print(f"üîç Found metadata: {metadata}")
                            if "json_output_file" in metadata:
                                json_filename = metadata["json_output_file"]
                                json_content_type = metadata.get(
                                    "json_content_type", "application/json"
                                )

                                print(f"üéØ Processing JSON file: {json_filename}")

                                # Get JSON file info
                                json_file_size, _ = await self._get_file_info(
                                    json_filename
                                )
                                # Use full filename as file_id for JSON files to avoid conflicts
                                json_file_id = json_filename

                                json_output_file = OutputFile(
                                    file_id=json_file_id,
                                    filename=json_filename,
                                    file_size=json_file_size,
                                    content_type=json_content_type,
                                )
                                output_files.append(json_output_file)
                                print(f"‚úÖ Added JSON file: {json_filename}")
                            else:
                                print("‚ùå No json_output_file in metadata")
                        else:
                            print("‚ùå No metadata in step_result")

                # Add final output if available and not already included
                if "final_output_path" in result and result["final_output_path"]:
                    final_path = result["final_output_path"]
                    filename = os.path.basename(final_path)

                    if filename:
                        # Check if this file is not already in output_files
                        existing_files = [f.filename for f in output_files]
                        if filename not in existing_files:
                            file_size, content_type = await self._get_file_info(
                                final_path
                            )

                            # MinIO„Å´‰øùÂ≠ò„Åï„Çå„Çã„Éï„Ç°„Ç§„É´ÂêçÔºàÊã°ÂºµÂ≠ê„Å™„ÅóÔºâ„Çífile_id„Å®„Åó„Å¶‰ΩøÁî®
                            file_id = os.path.splitext(filename)[0]

                            output_file = OutputFile(
                                file_id=file_id,
                                filename=filename,
                                file_size=file_size,
                                content_type=content_type,
                            )
                            output_files.append(output_file)

                execution.output_files = output_files

                logger.info(
                    f"Direct gRPC pipeline execution completed for {execution_id} in {result.get('execution_time_ms', 0):.2f}ms"
                )

            else:
                execution.status = ExecutionStatus.FAILED
                execution.error_message = result.get("error", "Unknown error")
                logger.error(
                    f"Direct gRPC pipeline execution failed for {execution_id}: {execution.error_message}"
                )

            await self._notify_execution_update(execution)

        except Exception as e:
            logger.error(f"Error in direct pipeline execution for {execution_id}: {e}")
            execution = self.executions.get(execution_id)
            if execution:
                execution.status = ExecutionStatus.FAILED
                execution.error_message = str(e)
                await self._notify_execution_update(execution)

    async def _build_pipeline_config(
        self, execution_request: ExecutionRequest, input_files: List[str]
    ) -> Dict[str, Any]:
        """Build pipeline configuration for direct gRPC execution"""
        from app.services.pipeline_service import PipelineService
        from app.database import get_db

        # Get actual pipeline definition from database
        pipeline_service = PipelineService()
        db_gen = get_db()
        db = await db_gen.__anext__()

        try:
            pipeline = await pipeline_service.get_pipeline(
                execution_request.pipeline_id, db
            )

            if not pipeline:
                # Fallback to default pipeline if not found
                logger.warning(
                    f"Pipeline {execution_request.pipeline_id} not found, using fallback"
                )
                return self._get_fallback_pipeline_config(
                    execution_request, input_files
                )

            # Build steps from pipeline components in the defined order
            steps = []
            for i, component in enumerate(pipeline.components):
                step_id = f"{component.component_type}-step-{i}"

                # Merge component parameters with execution request parameters
                merged_parameters = component.parameters.copy()
                merged_parameters.update(execution_request.parameters)

                # Set up dependencies based on component order
                dependencies = []
                if i > 0:
                    # Each component depends on the previous one (simple sequential execution)
                    previous_component = pipeline.components[i - 1]
                    dependencies = [f"{previous_component.component_type}-step-{i-1}"]

                step = {
                    "stepId": step_id,
                    "componentName": component.component_type,
                    "parameters": merged_parameters,
                    "dependencies": dependencies,
                }
                steps.append(step)

            return {
                "pipelineId": execution_request.pipeline_id,
                "steps": steps,
                "globalParameters": {
                    "inputPath": input_files[0] if input_files else None,
                    "executionId": execution_request.pipeline_id,
                },
            }

        except Exception as e:
            logger.error(f"Error building pipeline config: {e}")
            return self._get_fallback_pipeline_config(execution_request, input_files)
        finally:
            await db.close()

    def _get_fallback_pipeline_config(
        self, execution_request: ExecutionRequest, input_files: List[str]
    ) -> Dict[str, Any]:
        """Fallback pipeline configuration"""
        return {
            "pipelineId": execution_request.pipeline_id,
            "steps": [
                {
                    "stepId": "resize-step",
                    "componentName": "resize",
                    "parameters": {
                        "width": execution_request.parameters.get("target_width", 800),
                        "height": execution_request.parameters.get(
                            "target_height", 600
                        ),
                        "maintain_aspect_ratio": True,
                    },
                    "dependencies": [],
                },
                {
                    "stepId": "ai-detection-step",
                    "componentName": "ai_detection",
                    "parameters": {
                        "model_name": execution_request.parameters.get(
                            "model_name", "yolo"
                        ),
                        "confidence_threshold": execution_request.parameters.get(
                            "confidence_threshold", 0.5
                        ),
                        "draw_boxes": True,
                    },
                    "dependencies": ["resize-step"],
                },
                {
                    "stepId": "filter-step",
                    "componentName": "filter",
                    "parameters": {
                        "filter_type": execution_request.parameters.get(
                            "filter_type", "gaussian"
                        ),
                        "intensity": execution_request.parameters.get(
                            "filter_intensity", 1.0
                        ),
                    },
                    "dependencies": ["ai-detection-step"],
                },
            ],
            "globalParameters": {
                "inputPath": input_files[0] if input_files else None,
                "executionId": execution_request.pipeline_id,
            },
        }

    async def _notify_execution_update(self, execution: Execution):
        """Notify WebSocket clients of execution updates"""
        websocket_manager = self.get_websocket_manager()
        if websocket_manager:
            try:
                # Convert execution to dict for broadcasting
                execution_dict = execution.dict()
                await websocket_manager.broadcast_execution_update(
                    execution.execution_id, execution_dict
                )
            except Exception as e:
                logger.warning(f"Failed to broadcast execution update: {e}")

    async def _get_file_info(self, file_path: str) -> tuple[int, str]:
        """MinIO„Åã„Çâ„Éï„Ç°„Ç§„É´„Çµ„Ç§„Ç∫„Å®„Ç≥„É≥„ÉÜ„É≥„ÉÑ„Çø„Ç§„Éó„ÇíÂèñÂæó"""
        try:
            # FileService„Çí‰ΩøÁî®„Åó„Å¶MinIO„Åã„Çâ„Éï„Ç°„Ç§„É´ÊÉÖÂ†±„ÇíÂèñÂæó
            file_info = await self.file_service.get_file_info(file_path)

            if file_info:
                file_size = file_info.get("size", 0)
                content_type = file_info.get("content_type", "application/octet-stream")

                # „Éï„Ç°„Ç§„É´Êã°ÂºµÂ≠ê„Åã„Çâ„Ç≥„É≥„ÉÜ„É≥„ÉÑ„Çø„Ç§„Éó„ÇíÊé®ÂÆöÔºàMinIO„Åã„ÇâÂèñÂæó„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºâ
                if content_type == "application/octet-stream":
                    content_type = self._guess_content_type_from_filename(file_path)

                return file_size, content_type
            else:
                # „Éï„Ç°„Ç§„É´ÊÉÖÂ†±„ÅåÂèñÂæó„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
                return 0, self._guess_content_type_from_filename(file_path)

        except Exception as e:
            logger.warning(f"Failed to get file info for {file_path}: {e}")
            # „Ç®„É©„Éº„ÅÆÂ†¥Âêà„ÅØ„Éï„Ç°„Ç§„É´Âêç„Åã„ÇâÊé®ÂÆö
            return 0, self._guess_content_type_from_filename(file_path)

    def _guess_content_type_from_filename(self, filename: str) -> str:
        """„Éï„Ç°„Ç§„É´Âêç„Åã„Çâ„Ç≥„É≥„ÉÜ„É≥„ÉÑ„Çø„Ç§„Éó„ÇíÊé®ÂÆö"""
        import os

        ext = os.path.splitext(filename)[1].lower()
        content_type_map = {
            ".jpg": "image/jpeg",
            ".jpeg": "image/jpeg",
            ".png": "image/png",
            ".gif": "image/gif",
            ".bmp": "image/bmp",
            ".webp": "image/webp",
            ".tiff": "image/tiff",
            ".tif": "image/tiff",
            ".svg": "image/svg+xml",
            ".json": "application/json",
            ".txt": "text/plain",
            ".csv": "text/csv",
            ".xml": "application/xml",
        }

        return content_type_map.get(ext, "application/octet-stream")  # „Éá„Éï„Ç©„É´„Éà„ÇíÂ§âÊõ¥

    async def get_execution(self, execution_id: str) -> Optional[Execution]:
        """ÂÆüË°åÁä∂Ê≥Å„ÇíÂèñÂæó"""
        return self.executions.get(execution_id)

    async def cancel_execution(self, execution_id: str) -> bool:
        """ÂÆüË°å„Çí„Ç≠„É£„É≥„Çª„É´"""
        execution = self.executions.get(execution_id)
        if not execution:
            return False

        if execution.status in [ExecutionStatus.PENDING, ExecutionStatus.RUNNING]:
            execution.status = ExecutionStatus.CANCELLED

            # Kafka„Å´„Ç≠„É£„É≥„Çª„É´„É°„ÉÉ„Çª„Éº„Ç∏„ÇíÈÄÅ‰ø°
            cancel_message = {"execution_id": execution_id, "action": "cancel"}
            await self.kafka_service.send_message("execution-control", cancel_message)

            return True
        return False

    async def get_executions(
        self, limit: int = 100, offset: int = 0, pipeline_id: Optional[str] = None
    ) -> List[Execution]:
        """ÂÆüË°åÂ±•Ê≠¥„ÇíÂèñÂæóÔºà„Éë„Ç§„Éó„É©„Ç§„É≥ID„Åß„Éï„Ç£„É´„ÇøÂèØËÉΩÔºâ"""
        all_executions = list(self.executions.values())

        # „Éë„Ç§„Éó„É©„Ç§„É≥ID„Åß„Éï„Ç£„É´„Çø
        if pipeline_id:
            all_executions = [
                execution
                for execution in all_executions
                if execution.pipeline_id == pipeline_id
            ]

        # ‰ΩúÊàêÊó•ÊôÇ„Åß„ÇΩ„Éº„ÉàÔºàÊñ∞„Åó„ÅÑÈ†ÜÔºâ
        all_executions.sort(key=lambda x: x.created_at, reverse=True)
        return all_executions[offset : offset + limit]

    async def get_pending_executions(self) -> List[Execution]:
        """ÂÆüË°åÂæÖ„Å°„ÅÆÂÆüË°å„ÇíÂèñÂæóÔºàÁõ¥Êé•ÂÆüË°å„É¢„Éº„ÉâÁî®Ôºâ"""
        pending_executions = []
        for execution in self.executions.values():
            if execution.status == ExecutionStatus.PENDING:
                pending_executions.append(execution)
        return pending_executions

    async def get_running_executions(self) -> List[Execution]:
        """ÂÆüË°å‰∏≠„ÅÆÂÆüË°å„ÇíÂèñÂæóÔºà„ÉØ„Éº„ÇØ„Éï„É≠„ÉºÁõ£Ë¶ñÁî®Ôºâ"""
        running_executions = []
        for execution in self.executions.values():
            if execution.status == ExecutionStatus.RUNNING:
                running_executions.append(execution)
        return running_executions

    async def update_execution_status(
        self,
        execution_id: str,
        status: ExecutionStatus,
        progress_data: Optional[dict] = None,
    ):
        """ÂÆüË°åÁä∂Ê≥Å„ÇíÊõ¥Êñ∞ÔºàKafka„Ç≥„É≥„Ç∑„É•„Éº„Éû„Éº„Åã„ÇâÂëº„Å≥Âá∫„Åï„Çå„ÇãÔºâ"""
        execution = self.executions.get(execution_id)
        if not execution:
            return

        # „Çπ„ÉÜ„Éº„Çø„ÇπÂ§âÊõ¥ÊôÇ„Å´„Çø„Ç§„É†„Çπ„Çø„É≥„Éó„ÇíÊõ¥Êñ∞
        from datetime import datetime

        if execution.status != status:
            if status == ExecutionStatus.RUNNING and execution.started_at is None:
                execution.started_at = datetime.now(timezone.utc)
            elif status in [
                ExecutionStatus.COMPLETED,
                ExecutionStatus.FAILED,
                ExecutionStatus.CANCELLED,
            ]:
                # ÂÆå‰∫ÜÊôÇ„Å´started_at„ÅåË®≠ÂÆö„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÂ†¥Âêà„ÅØË®≠ÂÆö
                if execution.started_at is None:
                    execution.started_at = datetime.now(timezone.utc)
                execution.completed_at = datetime.now(timezone.utc)

        execution.status = status

        if progress_data:
            execution.progress.current_step = progress_data.get(
                "current_step", execution.progress.current_step
            )
            execution.progress.completed_steps = progress_data.get(
                "completed_steps", execution.progress.completed_steps
            )
            execution.progress.percentage = progress_data.get(
                "percentage", execution.progress.percentage
            )

        # WebSocket „ÅßÈÄ≤Êçó„Çí„É™„Ç¢„É´„Çø„Ç§„É†ÈÄöÁü•
        websocket_manager = self.get_websocket_manager()
        if websocket_manager:
            try:
                update_data = {
                    "execution_id": execution_id,
                    "status": status.value,
                    "progress": {
                        "current_step": execution.progress.current_step,
                        "completed_steps": execution.progress.completed_steps,
                        "total_steps": execution.progress.total_steps,
                        "percentage": execution.progress.percentage,
                    },
                }
                await websocket_manager.send_execution_update(execution_id, update_data)
            except Exception as e:
                print(f"Failed to send WebSocket update: {e}")

    async def fix_execution_timestamps(self, execution_id: str) -> bool:
        """Êó¢Â≠òÂÆüË°å„ÅÆ„Çø„Ç§„É†„Çπ„Çø„É≥„Éó„Çí‰øÆÊ≠£Ôºà„Éá„Éê„ÉÉ„Ç∞Áî®Ôºâ"""
        execution = self.executions.get(execution_id)
        if not execution:
            return False

        from datetime import datetime

        # „Çπ„ÉÜ„ÉÉ„Éó„Åã„ÇâÂÆüË°åÊôÇÈñì„ÇíÊé®ÂÆö
        if execution.steps and execution.status == ExecutionStatus.COMPLETED:
            step_start_times = [
                datetime.fromisoformat(s.started_at.replace("Z", "+00:00"))
                for s in execution.steps
                if s.started_at and s.status == "completed"
            ]
            step_end_times = [
                datetime.fromisoformat(s.completed_at.replace("Z", "+00:00"))
                for s in execution.steps
                if s.completed_at and s.status == "completed"
            ]

            if step_start_times and step_end_times:
                execution.started_at = min(step_start_times).replace(tzinfo=None)
                execution.completed_at = max(step_end_times).replace(tzinfo=None)
                return True

        return False
