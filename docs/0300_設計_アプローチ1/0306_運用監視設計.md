# 運用監視設計

# 文書管理情報

| 項目       | 内容                         |
| ---------- | ---------------------------- |
| 文書名     | ImageFlowCanvas 運用監視設計 |
| バージョン | 1.1                          |
| 作成日     | 2025年8月13日                |
| 更新日     | 2025年8月13日                |

# 1. 目的と方針
- 目的: 障害の早期検知と影響最小化、信頼性の継続的改善、運用負荷の平準化。
- 方針: SLO 駆動、ユーザー体験中心、ノイズ低減、段階的導入、標準化と自動化優先。
- 可観測性の三本柱: メトリクス、ログ、トレースを統合し、同一コンテキストで相互参照可能にする。

# 2. 監視対象と範囲
- 対象レイヤ: クライアント、API/アプリ、バッチ、ジョブキュー、DB、ストレージ、ネットワーク、CDN/外部依存、CI/CD。
- 対象イベント: デプロイ、スケール、構成変更、エラー、遅延、容量逼迫、セキュリティ事象、料金急騰。
- 環境: 本番、ステージング、開発（本番優先で詳細、下位環境は健全性中心）。

# 3. 信頼性目標（SLO/SLA）
| サービス | SLI                        | SLO             | 観測窓口         | 備考                    |
| -------- | -------------------------- | --------------- | ---------------- | ----------------------- |
| API      | 成功率（2xx/全リクエスト） | 99.9%/30日      | API ゲートウェイ | 重大障害は 5xx/分で補足 |
| API      | レイテンシ p95             | 300ms 以下/30日 | APM              | 経路別に閾値差分可      |
| バッチ   | ジョブ成功率               | 99.5%/30日      | ジョブランナー   | 期限内完了を併記        |
| DB       | エラーレート               | 0.1% 未満/30日  | DB メトリクス    | スロークエリ比率も併記  |
| フロント | 表示成功率                 | 99.9%/30日      | 合成監視         | 主要 3 パスを対象       |

- エラーバジェット: SLO 逸脱時は新機能凍結・安定化作業を優先。

# 4. 指標設計
## 4.1 メトリクス（RED/USE を意識）
| カテゴリ | 指標例             | 目的               | 可視化                 |
| -------- | ------------------ | ------------------ | ---------------------- |
| 受信量   | RPS、キュー長      | 需要把握、容量判断 | 時系列、ヒートマップ   |
| 成功率   | 成功割合、エラー率 | 体験健全性         | 集計、SLO バーンダウン |
| 遅延     | p50/p90/p95/p99    | 劣化検知           | 分位点、散布図         |
| 資源     | CPU/メモリ/FD/IO   | サチり検知         | ダッシュボード         |
| デプロイ | 変更件数、失敗率   | 変更失敗検知       | デプロイ直後比較       |

## 4.2 ログ
- 目的: 事象の事後分析、相関分析、監査。
- 要件: 構造化、リクエスト ID 付与、PII マスキング、サンプリング方針、保持期間とアーカイブ。
- 主要ログ種別: アプリイベント、アクセス、監査、ジョブログ、セキュリティ、インフラ。

## 4.3 分散トレース
- トレース ID をログ・メトリクスと関連付け、入口から下流までのレイテンシ分解を可能にする。
- 重要エンドポイントと重い下流呼び出しを重点サンプリング。

## 4.4 OpenTelemetry 方針（ログ/メトリクス/トレース）
- 目的: 全サービスを統一仕様で計測し、コンテキストを横断可能にする。
- 規格: OpenTelemetry（OTel）を採用。W3C Trace Context に準拠した伝播（traceparent, tracestate）。
- インスツルメンテーション対象: Backend API（FastAPI/gRPC）、gRPC常駐サービス、バッチ、フロント（必要箇所）、Ingest/Recorder/Extractor/Distributor。
- 相関: ログへ `trace_id`/`span_id` を自動付与（OTel Log Bridge）。リクエストID/ユーザーID等はプライバシー配慮の上タグ化。
- サンプリング: ヘッドベース10%を初期値、エラー/重要経路は100%優先。将来はゲートウェイCollectorでテールベース導入可。
- リソース属性: `service.name`、`service.version`、`deployment.environment`、`cloud.region` を必須とする。
- セマンティック規約: HTTP/gRPC/DB/Messaging など OTel Semantic Conventions に準拠。

# 5. アラート設計
## 5.1 優先度と基準
| 優先度  | 対象                           | 例                               | 期待応答             |
| ------- | ------------------------------ | -------------------------------- | -------------------- |
| P1 重大 | 全ユーザー影響、SLO 著しい逸脱 | 成功率急落、全滅、重要合成失敗   | 即時 24/7、5分内確認 |
| P2 高   | 一部ユーザー影響、劣化         | p95 悪化継続、部分的 5xx 増      | 当番 15分内調査      |
| P3 中   | 将来リスク                     | ディスク 80% 越え、キュー滞留    | 営業時間内対応       |
| P4 低   | 情報/運用                      | バックアップ完了、ジョブ遅延軽微 | 週次レビュー         |

- 原則: 人を起こすアラートは少数精鋭。閾値は可観測な影響起点に合わせる。

## 5.2 しきい値・時間窓
- エラーレート: 連続 5 分で 3% 超など、短期スパイク回避のための時間窓を設定。
- レイテンシ: p95 300ms 超が 10 分継続時に発火。
- 容量: 事前警告（70%）と行動閾値（85%）の二段階。

## 5.3 ノイズ低減
- 無駄な重複抑制、アラート結合、メンテナンスウィンドウ、抑止ルール、一次自己回復の待機。

## 5.4 通知チャネル
| 目的     | チャネル      | 備考                     |
| -------- | ------------- | ------------------------ |
| 即時喚起 | ページャ/電話 | P1 のみ                  |
| 協調対応 | チャット      | アラートスレッド自動生成 |
| 記録     | チケット      | 事後トラッキング         |
| レポート | メール        | 定期集計                 |

# 6. ダッシュボード設計
- レイヤ単位とユーザー体験軸の 2 系列で用意。
- リリース・インシデント時に使う「戦闘ダッシュボード」は簡潔に。

| ダッシュボード | 主指標               | 二次指標             | 利用シーン   |
| -------------- | -------------------- | -------------------- | ------------ |
| SRE 概観       | 成功率、p95、RPS     | 依存サービス健全性   | 常時監視     |
| API 詳細       | エンドポイント別 p95 | 5xx、スロークエリ    | 調査         |
| バッチ/キュー  | 成功率、滞留         | リトライ率、実行時間 | 夜間監視     |
| DB/キャッシュ  | CPU、接続、ヒット率  | ロック/待機          | 容量計画     |
| フロント体験   | 表示成功率、LCP      | JS エラー            | 合成監視連動 |

# 7. ログ管理・保持
| 種別         | 保持期間 | アーカイブ  | マスキング  |
| ------------ | -------- | ----------- | ----------- |
| アクセス     | 90 日    | 以降 1 年   | IP 準匿名化 |
| アプリ       | 30 日    | 以降 180 日 | PII 除去    |
| 監査         | 1 年     | 以降 3 年   | 改ざん検知  |
| セキュリティ | 180 日   | 以降 1 年   | 高感度      |

- 探索性とコストのバランスを取り、ホット/ウォーム/コールド階層化。

## 7.1 Kafka を用いたテレメトリ保存（短中期バッファ）
- 目的: 高スループットでの受け皿・再配信・短中期保管を Kafka で担保。
- Collector から Kafka へ送信し、トピックごとに保持・圧縮・スキーマを統一する。

| 種別     | トピック名            | キー/パーティション例              | 推奨保持            | 圧縮 | 備考                                   |
| -------- | --------------------- | ---------------------------------- | ------------------- | ---- | -------------------------------------- |
| ログ     | `telemetry.logs`      | key=`service.name`、env、日付      | 7–14 日             | on   | JSON/OTLP Logs、PII マスキング前提     |
| メトリクス | `telemetry.metrics`   | key=`service.name`、metric 名      | 48–72 時間          | on   | OTLP Metrics（デルタ/累積対応）        |
| トレース | `telemetry.traces`    | key=`trace_id`（順序性不要）       | 24–48 時間          | on   | OTLP Spans、エラーは優先長め保持       |

- 長期保存は S3 互換（MinIO）へ日次ローテーションでエクスポートし、ダッシュボード/検索系はそちらを参照。

## 7.2 Kafka Connect による長期保存（MinIO 連携）
- Sink: S3 互換 Sink（MinIO）で `s3://telemetry/{env}/{type}/{yyyy}/{MM}/{dd}/` に書き出し。
- 形式: Parquet（推奨、圧縮・スキーマ進化に強い）または JSON Lines（簡便）。
- パーティショニング: `env/service.name/date/hour` で Hive 互換ディレクトリ構造。
- 保持: 本ドキュメントの「ログ管理・保持」表の期間に準拠（例: アクセス=90日、アプリ=30日、監査=1年、セキュリティ=180日）。
- 監査・改ざん防止: 監査系はバージョニング/オブジェクトロック（WORM）運用を適用可能とする。

# 8. セキュリティ監視
- 失敗ログイン連続、権限昇格、重要設定変更、脆弱性検知、WAF ブロック、データ流出兆候。
- アラートはインシデント対応手順と連携、フォレンジック前提での保持と改ざん防止。

# 9. キャパシティとコスト監視
- 需要予測、オートスケール動作、ユニットコスト（例: 1 千リクエスト単価）を可視化。
- しきい値越え時は計画タスクを自動発行（拡張、削減、予約/前払い検討）。

# 10. 運用フローとエスカレーション
## 10.1 検知から復旧まで
1. 検知（自動アラート/申告）
2. 一次対応（暫定封じ込め、ユーザー告知判断）
3. エスカレーション（当番→担当チーム→責任者）
4. 恒久対応（原因分析、対策、検証）
5. 振り返り（事後レビュー、アクション登録と追跡）

## 10.2 エスカレーション表
| フェーズ | 担当      | 目標時間 | 成果物               |
| -------- | --------- | -------- | -------------------- |
| 検知     | 当番      | 5 分     | チケット起票         |
| 調査     | 当番+担当 | 30 分    | 影響評価、暫定策     |
| 復旧     | 担当      | 2 時間   | サービス回復         |
| 事後     | SRE       | 5 営業日 | レポート、アクション |

## 10.3 オンコール
- 24/7 体制、追い風シフト、交代時の引き継ぎチェックリスト。

# 11. 変更監視（デプロイ/設定）
- デプロイ直後はカナリア比較と衛星指標（成功率、p95、エラー率）を強調表示。
- 重要フラグ/設定変更は監査ログと相関し、リカバリ手順をランブック化。

# 12. DR/BCP・バックアップ監視
- バックアップ成功率、リストア検証頻度、RPO/RTO の測定とアラート。
- フェイルオーバー演習を定期実施し、ダッシュボードで「切替体験」を疑似再生。

# 13. 可観測性データフロー（文章による図解）
- クライアント/エッジ/サーバは OpenTelemetry で OTLP を出力し、ノード常駐の OTel Collector（Agent）へ送信する。
- Collector（Agent）はローカル集約・加工後、Cluster 内の Collector（Gateway）へフォワードする。
- Collector（Gateway）が Kafka Exporter で `telemetry.logs / telemetry.metrics / telemetry.traces` へ書き込み、短中期保管と再配信のハブとする。
- Kafka Connect が MinIO へ日次でエクスポートし、可視化/検索/分析は MinIO 上の Parquet/JSON を参照する。
- ルールエンジン/アラートは Kafka ストリーム/Collector Processor で閾値比較し、通知チャネルへ配信する。
- すべてのデータは `trace_id` をキーにログ・メトリクス・トレースを横断参照可能とする。

## 13.1 Collector 配置と冗長化
- 配置: K3s 上に DaemonSet（Agent）＋ Deployment（Gateway）。オンプレ/VM0 にも軽量 Collector を配置可。
- 可用性: Gateway は 2 インスタンス以上の冗長。Kafka ブローカ複数台と合わせて単一障害点を排除。
- セキュリティ: mTLS（OTLP over gRPC/HTTP）、Kafka 認証（SASL/SCRAM など）、転送時の暗号化を徹底。

## 13.2 サンプリング/集約ポリシー
- トレース: デフォルト10%ヘッドサンプリング＋エラー/重要経路は100%。必要に応じてテールベースへ拡張。
- メトリクス: スクラップ/プッシュの両対応（OTLP Metrics）。分位点や集約は Collector 側で計算可能。
- ログ: 構造化ログを OTel 形式で取り込み、PII は発行元でマスキング。高ボリュームはレート制御。

# 14. テストと演習
- 合成監視: 主要ユーザーフローの定期実行と成績表化。
- ゲームデー: 重大系の想定事象をシナリオ化し、検知〜復旧を訓練。
- アラート演習: 抑止・誤検知・重複を定期棚卸し、TTD/MTTA/MTTR を可視化。

# 15. 導入ステップとマイルストーン
| フェーズ       | 期間   | 成果                             |
| -------------- | ------ | -------------------------------- |
| 基盤整備       | 1 か月 | データ収集/可視化の最小セット    |
| SLO 導入       | 1 か月 | 主要 SLI/SLO とレポート          |
| アラート最適化 | 継続   | ノイズ半減、P1 応答目標達成      |
| 運用定着       | 継続   | ランブック整備、ゲームデー定例化 |

# 16. リスク・前提
- 外部依存の可視性不足、テナント制限、コスト変動、サンプリングによる盲点。
- 前提: チーム横断の責任分担、監視ルール変更のレビュー運用、個人情報保護遵守。

# 17. ランブック（骨子）
- 目的、想定事象、初動、切り分け、暫定回避、恒久対応、ロールバック、連絡・広報、検証、事後記録。

# 付録 A: メトリクス一覧（例）
| 名称               | 説明           | 粒度 | タグ例                   |
| ------------------ | -------------- | ---- | ------------------------ |
| api.success_rate   | 成功率         | 1 分 | サービス、エンドポイント |
| api.latency.p95    | p95 レイテンシ | 1 分 | サービス、エンドポイント |
| worker.queue_depth | キュー長       | 1 分 | キュー名                 |
| db.errors          | DB エラー率    | 1 分 | クラスタ、ロール         |
| cache.hit_ratio    | ヒット率       | 1 分 | クラスタ                 |

# 付録 B: ログイベント一覧（例）
| カテゴリ     | 主要フィールド                 | 利用目的         |
| ------------ | ------------------------------ | ---------------- |
| アクセス     | 方法、経路、結果、遅延、ID     | 体験、遅延、相関 |
| アプリ       | レベル、コード、メッセージ、ID | 失敗分析         |
| 監査         | 主体、操作、対象、結果、時刻   | 追跡、法令対応   |
| セキュリティ | 検知種別、深刻度、詳細         | インシデント対応 |

# 付録 C: 通知先一覧（例）
| 用途         | 宛先             | 備考           |
| ------------ | ---------------- | -------------- |
| P1           | ページャ番号     | 当番向け       |
| P2           | チャネル A       | チーム全体     |
| 週次レポート | メーリングリスト | 事業責任者含む |

---
更新履歴: 2025-08-12 初版作成（文章・図表のみ、コードなし）
