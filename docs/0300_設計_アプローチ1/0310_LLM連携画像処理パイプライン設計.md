# ImageFlowCanvas LLM連携画像処理パイプライン設計書

# 文書管理情報

| 項目 | 内容 |
| --- | --- |
| 文書名 | ImageFlowCanvas LLM連携画像処理パイプライン設計書 |
| バージョン | 0.1 |
| 作成日 | 2025年8月27日 |
| 更新日 | 2025年8月27日 |

---

# 1. 概要
## 1.1 目的
LLMに日本語で指示を与えるだけで、画像処理ツールを組み合わせたパイプラインを自動生成・実行する仕組みを設計する。

## 1.2 背景
既存システムではgRPC経由で定義済みパイプラインを高速実行しているが、構築は手作業で行われる【F:backend/app/services/grpc_pipeline_executor.py†L160-L200】【F:backend/app/services/pipeline_service.py†L1-L80】。自然言語からパイプラインを自動生成することで、非エンジニアでも柔軟な処理チェーンを構築できるようにする。

# 2. システム構成
- **LLMフロントエンド**: ユーザーの日本語指示をLLMへ送信し、パイプライン定義を生成。
- **ツールレジストリ**: 既存gRPCマイクロサービス（resize、ai-detection等）をメタデータ付きで管理。
- **パイプライン生成サービス**: LLM出力を解析し、MLRun用のワークフロー定義を生成して保存。
- **MLRunオーケストレータ**: 生成されたワークフローを実行し、ステップごとの状態管理を行う。
- **Triton Inference Server**: モデル推論ステップを高速に提供し、MLRunからgRPC/HTTPで呼び出される。
- **既存Pipeline Executor**: MLRun完了後、結果を既存の高速gRPC実行経路へ渡し、レスポンスを統一。

# 3. 主要コンポーネント
## 3.1 LLMアダプタ
OpenAI互換APIを想定。ツール情報をプロンプトに含め、ステップとパラメータを抽出する。

## 3.2 パイプライン定義生成
LLM出力をJSONスキーマに整形し、`PipelineService`を通じて永続化する。

## 3.3 MLRun
各ステップをMLRunの関数として登録。依存関係を解析し、並列実行や再実行を制御する。

## 3.4 Triton Inference Server
AI推論用モデルをホスト。MLRunから`gRPC`/`HTTP`で呼び出し、レスポンスを標準化する。

## 3.5 既存Pipeline Executor
生成済みパイプラインを直接gRPCで実行し、進捗通知やエラーハンドリングを既存フレームワークに統合する。

# 4. 処理フロー
1. ユーザーが日本語で処理内容を入力。
2. LLMが指示を解析し、ステップ列とパラメータを出力。
3. パイプライン生成サービスがMLRunワークフローとTritonモデル参照を含む定義を作成。
4. 定義は`PipelineService`に保存され、IDが返却される。
5. 実行要求を受けたMLRunがワークフローを実行し、必要に応じてTritonへ推論リクエスト。
6. 実行結果は既存Pipeline Executor経由でクライアントへ返却される。

# 5. デプロイ・運用
- MLRunとTritonは処理層(K3s/Nomad)にデプロイし、GPUリソースを共有する。
- ツールレジストリとパイプライン定義はPostgreSQLに保持し、バックアップ対象とする。
- 生成処理と実行処理のログは既存の監視基盤へ送信し、トレースを可能にする。

# 6. セキュリティ
- LLM APIキーはSecret Managerで管理。
- 生成されたパイプラインはサンドボックス化され、許可されたツールのみ実行可能。

# 7. 今後の課題
- LLMの出力安定性向上と誤解釈対策。
- ユーザーによるツール拡張時のスキーマ管理。
- 大規模モデルのロード時間最適化。

