# ImageFlowCanvas LLM連携画像処理パイプライン設計書

# 文書管理情報

| 項目       | 内容                                              |
| ---------- | ------------------------------------------------- |
| 文書名     | ImageFlowCanvas LLM連携画像処理パイプライン設計書 |
| バージョン | 0.2                                               |
| 作成日     | 2025年8月27日                                     |
| 更新日     | 2025年8月31日                                     |

---

# 1. 概要
## 1.1 目的
LLMに日本語で指示を与えるだけで、画像処理ツールを組み合わせたパイプラインを自動生成・実行する仕組みを設計する。

## 1.2 背景
既存システムではgRPC経由で定義済みパイプラインを高速実行しているが、構築は手作業で行われる。自然言語からパイプラインを自動生成することで、非エンジニアでも柔軟な処理チェーンを構築できるようにする。

# 2. システム構成
- **LLMフロントエンド**: ユーザーの日本語指示をLLMへ送信し、パイプライン定義を生成。
- **ツールレジストリ**: 既存gRPCマイクロサービス（resize、ai-detection 等）をメタデータ付きで管理。
- **パイプライン生成サービス**: LLM出力を解析し、Backend のパイプライン定義スキーマに整形して保存。
- **パイプライン実行エンジン（Backend API）**: Backend の `GRPCPipelineExecutor` が定義に従って各 gRPC サービスを直接呼び出し、並列化・リトライ・進捗通知（Kafka 任意）を行う。
- **Triton Inference Server**: モデル推論ステップを高速に提供し、Backend や gRPC サービスから gRPC/HTTP で呼び出される。
- **既存 Pipeline Executor**: 上記パイプライン実行エンジンの実体（高速 gRPC 実行経路）。

# 3. 主要コンポーネント
## 3.1 LLMアダプタ
OpenAI互換APIを想定。ツール情報をプロンプトに含め、ステップとパラメータを抽出する。

## 3.2 パイプライン定義生成
LLM出力を Backend のパイプライン定義スキーマに整形し、`PipelineService` を通じて永続化する。

## 3.3 パイプライン実行エンジン（Backend API）
Backend に実装された `GRPCPipelineExecutor` が、パイプライン定義（ステップ列・依存関係・パラメータ）を元に以下を担う。

- 依存解決: トポロジカルソートにより実行順を決定、並列可能なステップは同一グループとして同時実行。
- 実行方式: 既定は低レイテンシな「DIRECT_GRPC」。
- 呼び出し: `resize-grpc` / `ai-detection-grpc` / `filter-grpc` などの常駐 gRPC サービスに直接 RPC。推論は Triton を経由。
- 監視・通知: 各ステップ開始/完了/失敗を Kafka トピックへ通知（任意）。メトリクスは OTel で収集。
- 障害時挙動: ステップ単位のリトライ／フェイルファスト／部分成功扱いなど、パイプライン方針に従う。

## 3.4 Triton Inference Server
 AI 推論用モデルをホスト。Backend／gRPC サービスから `gRPC`/`HTTP` で呼び出し、レスポンスを標準化する。

## 3.5 既存Pipeline Executor
生成済みパイプラインを直接 gRPC で実行し、進捗通知やエラーハンドリングを既存フレームワークに統合する。これは本設計の「パイプライン実行エンジン（Backend API）」と同一コンポーネントである。

# 4. 処理フロー
1. ユーザーが日本語で処理内容を入力。
2. LLM が指示を解析し、ステップ列とパラメータを出力。
3. パイプライン生成サービスが Backend 用のパイプライン定義（Triton 参照を含む）を作成。
4. 定義は `PipelineService` に保存され、ID が返却される。
5. 実行要求を受けた Backend の `GRPCPipelineExecutor` が定義に従って各 gRPC サービスを実行し、必要に応じて Triton へ推論リクエスト。
6. 実行結果は Backend 経由でクライアントへ返却される（進捗イベントは Kafka 配信／WebSocket 等で通知）。

# 5. デプロイ・運用
- Triton と gRPC マイクロサービスは処理層（K3s または Nomad）にデプロイし、GPU リソースを共有する。
- Backend（パイプライン実行エンジン）・ツールレジストリ・パイプライン定義は PostgreSQL を利用し、バックアップ対象とする。
- 生成処理と実行処理のログ／メトリクスは既存の監視基盤へ送信し、トレーサビリティを担保する。

# 6. セキュリティ
- LLM APIキーはSecret Managerで管理。
- 生成されたパイプラインはサンドボックス化され、許可されたツールのみ実行可能。

# 7. 今後の課題
- LLM の出力安定性向上と誤解釈対策。
- ユーザーによるツール拡張時のスキーマ管理。
